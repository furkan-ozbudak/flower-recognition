# -*- coding: utf-8 -*-
"""project-3-InceptionV3-experiment-1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1neL1dlMXAv6-BSEyIAY65gLmtUjDD4iT

# InceptionV3: Experiment - 1
"""

from google.colab import drive
drive.mount('/content/gdrive')

train_data_dir = '/content/gdrive/My Drive/ML/project-3/flowers'
img_width, img_height = 299, 299 
batch_size = 32
EPOCHS = 10

from keras.preprocessing.image import ImageDataGenerator
image_datagen = ImageDataGenerator(
    rescale=1./255, 
    vertical_flip = True,
    horizontal_flip = True,
    rotation_range=20,
    shear_range=0.05,
    zoom_range=0.2,
    width_shift_range=0.1,
    height_shift_range=0.1,
    validation_split=0.2,
    #channel_shift_range=0.1
)

train_gen = image_datagen.flow_from_directory(
        train_data_dir, 
        target_size=(img_height, img_width), 
        batch_size=batch_size, 
        class_mode="categorical", 
        subset="training")

valid_gen = image_datagen.flow_from_directory(
        train_data_dir, 
        target_size=(img_height, img_width), 
        batch_size=batch_size, 
        class_mode="categorical", 
        subset="validation")

from keras.callbacks import EarlyStopping, TensorBoard, CSVLogger, ReduceLROnPlateau, ModelCheckpoint

# Callbacks - Get some information while the model is training.

# Stop training when a monitored metric has stopped improving.
earlystop = EarlyStopping(
    monitor='val_loss',
    min_delta=0.001,
    patience=10,
    verbose=1,
    mode='auto' 
)

# Callback that streams epoch results to a csv file.
csvlogger = CSVLogger( 
    filename= "training_csv.log",
    separator = ",",
    append = False
)

# Reduce learning rate when a metric has stopped improving.
reduce = ReduceLROnPlateau( 
    monitor='val_loss',
    factor=0.1, 
    patience=3,
    verbose=1, 
    mode='auto',
)

"""Hyperparameters"""

from keras.applications.inception_v3 import InceptionV3
base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))
print('Loaded model!')

"""**Freeze** the layers in base_model"""

for layer in base_model.layers:
    layer.trainable = False

"""# HYPER-PARAMETERS"""

second_dense_512 = [0, 1]
dropout = [0, 1]

import time
from keras.layers import Dense,Flatten,Dropout,Concatenate,GlobalAveragePooling2D,BatchNormalization,Activation
from keras.models import Sequential,Input,Model

for dense2 in second_dense_512:
    for drop in dropout:
        
        NAME = "flowers-inception-dense{}-drop{}-{}".format(dense2, drop, int(time.time()))
        print(NAME)
        logdir = "logs/flowers-inception/{}/".format(NAME)
        
        # ModelCheckpoint - Callback to save the Keras model or model weights at some frequency.
        checkpoint = ModelCheckpoint(
            '{}base.model'.format(logdir),
            monitor='val_loss',
            mode='min',
            save_weights_only=True,
            save_best_only = True,
            verbose = 1)

        # TensorBoard provides the visualization and tooling needed for machine learning experimentation:
        tensorboard = TensorBoard(
            log_dir = logdir,
            histogram_freq=0,
            batch_size=batch_size,
            write_graph=True,
            write_grads=True,
            write_images=False,
        )

        x = base_model.output
        x = GlobalAveragePooling2D()(x)
        x = Dense(1024)(x)
        x = BatchNormalization()(x)
        x = Activation("relu")(x)
        if drop == 1 : x = Dropout(0.3)(x)
        if dense2 == 1 : 
            x = Dense(512)(x)
            x = BatchNormalization()(x)
            x = Activation("relu")(x)
            if drop == 1 : x = Dropout(0.3)(x)
        
        predictions = Dense(5, activation='softmax')(x)
        
        model = Model(base_model.input, predictions)
        
        
        model.compile(loss='categorical_crossentropy',
                      optimizer='Adam',
                      metrics=['accuracy'])

import pandas as pd
pd.set_option('max_colwidth', -1)
layers = [(layer, layer.name, layer.trainable) for layer in model.layers]
print(pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable']) )

history = model.fit_generator(
            train_gen,
            steps_per_epoch = train_gen.n // train_gen.batch_size, # normalde len(X_train) / batch_size,
            epochs= EPOCHS,
            validation_data = valid_gen,
            validation_steps=valid_gen.n // valid_gen.batch_size, # normalde len(X_valid) / batch_size,
            verbose=1,
            callbacks=[checkpoint,tensorboard,csvlogger,reduce,earlystop])

"""# Save the model"""

# model.save('flower_inceptionV3.h5')

"""# Learning plots"""

import matplotlib.pyplot as plt

# plot loss and accuracy image
history_dict = history.history
train_loss = history_dict["loss"]
train_accuracy = history_dict["accuracy"]
val_loss = history_dict["val_loss"]
val_accuracy = history_dict["val_accuracy"]

# figure 1
plt.figure()
plt.plot(range(EPOCHS), train_loss, label='train_loss')
plt.plot(range(EPOCHS), val_loss, label='val_loss')
plt.legend()
plt.xlabel('epochs')
plt.ylabel('loss')

# figure 2
plt.figure()
plt.plot(range(EPOCHS), train_accuracy, label='train_accuracy')
plt.plot(range(EPOCHS), val_accuracy, label='val_accuracy')
plt.legend()
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.show()

"""# References

https://www.kaggle.com/emirhanozkan/flower-classification-inceptionv3

https://www.kaggle.com/yaoyi970403/flowers-rrecognition-project-acc-96-6

https://www.kaggle.com/shivamb/cnn-architectures-vgg-resnet-inception-tl

https://www.kaggle.com/rajmehra03/flower-recognition-cnn-keras
"""